{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kerasでは下記のようなこともできます\n",
    "\n",
    "## ①学習の可視化\n",
    "## ②モデルの保存と読み込み\n",
    "## ③コールバック\n",
    "### 　　　1.学習過程の可視化\n",
    "### 　　　2.学習の早期終了\n",
    "## ④最適なモデルの選択"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 連続値データの読み込み\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "# 訓練データとテストデータに分ける\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris['data'], iris['target'], test_size=0.3,  random_state=0)\n",
    "\n",
    "\"\"\"\n",
    "# 標準化（Standardization）\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test) \n",
    "scaler = StandardScaler()\n",
    "y_train_s = scaler.fit_transform(y_train.reshape(len(y_train),1))\n",
    "y_test_s = scaler.transform(y_test.reshape(len(y_test),1)) \n",
    "\"\"\"\n",
    "\n",
    "# 正規化（Normarization）\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler_x = MinMaxScaler()\n",
    "X_train_n = scaler_x.fit_transform(X_train)\n",
    "X_test_n = scaler_x.transform(X_test) \n",
    "\n",
    "# one-hotベクトル化\n",
    "import keras\n",
    "y_train = keras.utils.to_categorical(y_train,3)\n",
    "y_test = keras.utils.to_categorical(y_test,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ①学習の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要ライブラリのインポート\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.layers import normalization\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "105/105 [==============================] - 0s 3ms/step - loss: 1.2439 - acc: 0.4190\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.4106 - acc: 0.7048\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.3031 - acc: 0.8190\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.2454 - acc: 0.8857\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.1631 - acc: 0.8571\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.1412 - acc: 0.8667\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.1464 - acc: 0.9238\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.1460 - acc: 0.9048\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.1299 - acc: 0.9143\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.1212 - acc: 0.8857\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - 0s 66us/step - loss: 0.1179 - acc: 0.8857\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0909 - acc: 0.9238\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0891 - acc: 0.9048\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.1057 - acc: 0.9333\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0959 - acc: 0.9429\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.1035 - acc: 0.8952\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0701 - acc: 0.9619\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - 0s 66us/step - loss: 0.0873 - acc: 0.9714\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0750 - acc: 0.9333\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0805 - acc: 0.9429\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.1017 - acc: 0.9048\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0584 - acc: 0.9905\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0664 - acc: 0.9619\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0761 - acc: 0.9619\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - 0s 66us/step - loss: 0.0495 - acc: 0.9810\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0770 - acc: 0.9524\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0555 - acc: 0.9714\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0559 - acc: 0.9619\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - 0s 66us/step - loss: 0.0629 - acc: 0.9619\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0652 - acc: 0.9524\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0523 - acc: 0.9810\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0618 - acc: 0.9333\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0511 - acc: 0.9619\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0594 - acc: 0.9524\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - 0s 84us/step - loss: 0.0471 - acc: 0.9810\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0473 - acc: 0.9714\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0520 - acc: 0.9714\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0521 - acc: 0.9524\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0521 - acc: 0.9524\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0519 - acc: 0.9810\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0462 - acc: 0.9810\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0458 - acc: 0.9810\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0438 - acc: 0.9810\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0495 - acc: 0.9810\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0558 - acc: 0.9524\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0529 - acc: 0.9810\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0545 - acc: 0.9524\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0476 - acc: 0.9619\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0401 - acc: 0.9810\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0459 - acc: 0.9619\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0358 - acc: 0.9905\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0408 - acc: 0.9619\n",
      "Epoch 53/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0441 - acc: 0.9619\n",
      "Epoch 54/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0339 - acc: 0.9905\n",
      "Epoch 55/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0474 - acc: 0.9524\n",
      "Epoch 56/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0422 - acc: 0.9714\n",
      "Epoch 57/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0467 - acc: 0.9524\n",
      "Epoch 58/100\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0373 - acc: 0.9714\n",
      "Epoch 59/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0408 - acc: 0.9905\n",
      "Epoch 60/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0444 - acc: 0.9714\n",
      "Epoch 61/100\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0306 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0394 - acc: 0.9810\n",
      "Epoch 63/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0442 - acc: 0.9810\n",
      "Epoch 64/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0376 - acc: 0.9905\n",
      "Epoch 65/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0344 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "105/105 [==============================] - 0s 57us/step - loss: 0.0411 - acc: 0.9810\n",
      "Epoch 67/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0463 - acc: 0.9714\n",
      "Epoch 68/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0585 - acc: 0.9524\n",
      "Epoch 69/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0581 - acc: 0.9429\n",
      "Epoch 70/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0451 - acc: 0.9714\n",
      "Epoch 71/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0472 - acc: 0.9810\n",
      "Epoch 72/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0400 - acc: 0.9905\n",
      "Epoch 73/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0463 - acc: 0.9714\n",
      "Epoch 74/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0264 - acc: 0.9905\n",
      "Epoch 75/100\n",
      "105/105 [==============================] - 0s 57us/step - loss: 0.0314 - acc: 0.9810\n",
      "Epoch 76/100\n",
      "105/105 [==============================] - 0s 57us/step - loss: 0.0259 - acc: 0.9810\n",
      "Epoch 77/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0399 - acc: 0.9619\n",
      "Epoch 78/100\n",
      "105/105 [==============================] - 0s 57us/step - loss: 0.0331 - acc: 0.9905\n",
      "Epoch 79/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0421 - acc: 0.9714\n",
      "Epoch 80/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0349 - acc: 0.9810\n",
      "Epoch 81/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0305 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0309 - acc: 0.9714\n",
      "Epoch 83/100\n",
      "105/105 [==============================] - 0s 57us/step - loss: 0.0308 - acc: 0.9905\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 67us/step - loss: 0.0311 - acc: 0.9905\n",
      "Epoch 85/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0505 - acc: 0.9238\n",
      "Epoch 86/100\n",
      "105/105 [==============================] - 0s 57us/step - loss: 0.0317 - acc: 0.9905\n",
      "Epoch 87/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0379 - acc: 0.9524\n",
      "Epoch 88/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0426 - acc: 0.9810\n",
      "Epoch 89/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0315 - acc: 0.9905\n",
      "Epoch 90/100\n",
      "105/105 [==============================] - 0s 57us/step - loss: 0.0312 - acc: 0.9810\n",
      "Epoch 91/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0288 - acc: 0.9810\n",
      "Epoch 92/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0375 - acc: 0.9619\n",
      "Epoch 93/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0253 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0350 - acc: 0.9619\n",
      "Epoch 95/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0258 - acc: 0.9905\n",
      "Epoch 96/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0290 - acc: 0.9905\n",
      "Epoch 97/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0237 - acc: 0.9905\n",
      "Epoch 98/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0291 - acc: 0.9905\n",
      "Epoch 99/100\n",
      "105/105 [==============================] - 0s 67us/step - loss: 0.0268 - acc: 0.9905\n",
      "Epoch 100/100\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0288 - acc: 0.9810\n"
     ]
    }
   ],
   "source": [
    "# モデル生成\n",
    "model = Sequential()\n",
    "\n",
    "# 層の追加\n",
    "layers=[\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.01),\n",
    "    normalization.BatchNormalization(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.01),\n",
    "    normalization.BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(3, activation='linear')\n",
    "]\n",
    "for layer in layers:\n",
    "    model.add(layer)\n",
    "\n",
    "# モデルの学習設定\n",
    "model.compile(\n",
    "    loss=losses.mean_squared_error,\n",
    "    optimizer=optimizers.Adam(),\n",
    "    metrics=['acc']\n",
    ")\n",
    "\n",
    "# モデルの学習\n",
    "result = model.fit(\n",
    "    X_train_n,\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 描画データ\n",
    "acc = result.history['acc']  # 学習データの正答率\n",
    "loss = result.history['loss']  # 学習データの損失\n",
    "\n",
    "# x軸\n",
    "epochs = range(len(acc))\n",
    "\n",
    "# 図示\n",
    "plt.plot(epochs, acc, label='Train Accuracy')\n",
    "plt.plot(epochs, loss, label='Train Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ②モデルの保存と読み込み\n",
    "\n",
    "学習したモデルは、保存できます。\n",
    "\n",
    "保存したモデルは、使用したいときにプログラムから読み込み利用できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの保存\n",
    "model.save('./model/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの読み込み\n",
    "model = keras.models.load_model('./model/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 2 1 0\n",
      " 1 1 1 2 0 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "# 予測値の算出\n",
    "import numpy as np\n",
    "y_predict = np.argmax(model.predict(X_test_n),axis=1)\n",
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ③コールバック\n",
    "\n",
    "コールバックは訓練中で適用される関数のことです。訓練中にモデル内部の状態と統計量を可視化する際に、コールバックを使います。その他にも、各エポック毎に「何かしらの処理」を行いたい場合、ここに記述します。\n",
    "\n",
    "\n",
    "> コールバックについて紹介する前に、データセットの種類について紹介します。\n",
    "> \n",
    "> + **訓練データ**：重み更新に使用する。\n",
    "> + **テストデータ**：モデル性能の検証に利用する。\n",
    "> + **検証データ(バリデーションデータ)**：学習の早期終了（EarlyStopping）やハイパーパラメータの調整に利用する\n",
    "> \n",
    "> ＊ ほとんどの場合、ハイパーパラメータの調整は、訓練データを用いて行うため、気にしなくてよいが、学術的なデータ解析の際には、検証用データも使用する。ほとんどの実務的な場合においては、検証データは、学習の早期終了などに用いられると考えてよい。\n",
    "\n",
    "\n",
    "### コールバックで出来ること\n",
    "\n",
    "**1. 学習の早期終了（EarlyStopping）**：バリデーションデータへの精度に基づいて、改善しなかった場合は、学習を早期に打ち切り、過学習の防止に務める。\n",
    "\n",
    "**2. 学習過程の保存（ModelCheckpoint）**：学習の途中経過を記録しておく。今まで見てきた手法では、学習終了時点のモデルを採用し、予測値の算出を行ったが、学習途中の精度が良い場合がある。そのような場合は、このModelCheckpointで記録しておいたモデルの中から、最も精度のいいモデルを採用する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.学習の早期終了（EarlyStopping）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 45 samples\n",
      "Epoch 1/300\n",
      "105/105 [==============================] - 0s 3ms/step - loss: 0.7677 - acc: 0.5333 - val_loss: 0.3198 - val_acc: 0.4444\n",
      "Epoch 2/300\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.3583 - acc: 0.8762 - val_loss: 0.3104 - val_acc: 0.2444\n",
      "Epoch 3/300\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.2750 - acc: 0.8952 - val_loss: 0.3098 - val_acc: 0.2444\n",
      "Epoch 4/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.2608 - acc: 0.7714 - val_loss: 0.3113 - val_acc: 0.2444\n",
      "Epoch 5/300\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.1869 - acc: 0.8286 - val_loss: 0.3066 - val_acc: 0.2444\n",
      "Epoch 6/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.1595 - acc: 0.9238 - val_loss: 0.3029 - val_acc: 0.2444\n",
      "Epoch 7/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.1147 - acc: 0.8857 - val_loss: 0.2982 - val_acc: 0.2444\n",
      "Epoch 8/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.1168 - acc: 0.9238 - val_loss: 0.2942 - val_acc: 0.4889\n",
      "Epoch 9/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0945 - acc: 0.9333 - val_loss: 0.2906 - val_acc: 0.6222\n",
      "Epoch 10/300\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0949 - acc: 0.9333 - val_loss: 0.2866 - val_acc: 0.6000\n",
      "Epoch 11/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0909 - acc: 0.9524 - val_loss: 0.2831 - val_acc: 0.4889\n",
      "Epoch 12/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0889 - acc: 0.9333 - val_loss: 0.2820 - val_acc: 0.4222\n",
      "Epoch 13/300\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0723 - acc: 0.9524 - val_loss: 0.2794 - val_acc: 0.4222\n",
      "Epoch 14/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.1056 - acc: 0.9143 - val_loss: 0.2760 - val_acc: 0.4000\n",
      "Epoch 15/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0850 - acc: 0.9429 - val_loss: 0.2744 - val_acc: 0.4000\n",
      "Epoch 16/300\n",
      "105/105 [==============================] - 0s 104us/step - loss: 0.0794 - acc: 0.9619 - val_loss: 0.2739 - val_acc: 0.4000\n",
      "Epoch 17/300\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0712 - acc: 0.9619 - val_loss: 0.2664 - val_acc: 0.4222\n",
      "Epoch 18/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0680 - acc: 0.9524 - val_loss: 0.2572 - val_acc: 0.4000\n",
      "Epoch 19/300\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0610 - acc: 0.9524 - val_loss: 0.2516 - val_acc: 0.4000\n",
      "Epoch 20/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0613 - acc: 0.9714 - val_loss: 0.2471 - val_acc: 0.4000\n",
      "Epoch 21/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0718 - acc: 0.9429 - val_loss: 0.2431 - val_acc: 0.4000\n",
      "Epoch 22/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0598 - acc: 0.9714 - val_loss: 0.2407 - val_acc: 0.4000\n",
      "Epoch 23/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0694 - acc: 0.9524 - val_loss: 0.2390 - val_acc: 0.4000\n",
      "Epoch 24/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0595 - acc: 0.9619 - val_loss: 0.2363 - val_acc: 0.4000\n",
      "Epoch 25/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0482 - acc: 0.9810 - val_loss: 0.2328 - val_acc: 0.4000\n",
      "Epoch 26/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0527 - acc: 0.9714 - val_loss: 0.2270 - val_acc: 0.4000\n",
      "Epoch 27/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0710 - acc: 0.9524 - val_loss: 0.2216 - val_acc: 0.4000\n",
      "Epoch 28/300\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0536 - acc: 0.9619 - val_loss: 0.2176 - val_acc: 0.4000\n",
      "Epoch 29/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0548 - acc: 0.9619 - val_loss: 0.2141 - val_acc: 0.4000\n",
      "Epoch 30/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0687 - acc: 0.9429 - val_loss: 0.2112 - val_acc: 0.4000\n",
      "Epoch 31/300\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0742 - acc: 0.9619 - val_loss: 0.2082 - val_acc: 0.4000\n",
      "Epoch 32/300\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0530 - acc: 0.9810 - val_loss: 0.2056 - val_acc: 0.4000\n",
      "Epoch 33/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0745 - acc: 0.9333 - val_loss: 0.2008 - val_acc: 0.4000\n",
      "Epoch 34/300\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0549 - acc: 0.9333 - val_loss: 0.2036 - val_acc: 0.4000\n",
      "Epoch 35/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0560 - acc: 0.9714 - val_loss: 0.1983 - val_acc: 0.4444\n",
      "Epoch 36/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0599 - acc: 0.9524 - val_loss: 0.1921 - val_acc: 0.6889\n",
      "Epoch 37/300\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0530 - acc: 0.9524 - val_loss: 0.1934 - val_acc: 0.6222\n",
      "Epoch 38/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0432 - acc: 1.0000 - val_loss: 0.1940 - val_acc: 0.4667\n",
      "Epoch 39/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0374 - acc: 1.0000 - val_loss: 0.1902 - val_acc: 0.4667\n",
      "Epoch 40/300\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0430 - acc: 0.9810 - val_loss: 0.1913 - val_acc: 0.4444\n",
      "Epoch 41/300\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0380 - acc: 0.9619 - val_loss: 0.1963 - val_acc: 0.4222\n",
      "Epoch 42/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0517 - acc: 0.9619 - val_loss: 0.1971 - val_acc: 0.4222\n",
      "Epoch 43/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0464 - acc: 0.9810 - val_loss: 0.1902 - val_acc: 0.4000\n",
      "Epoch 44/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0339 - acc: 0.9905 - val_loss: 0.1846 - val_acc: 0.4000\n",
      "Epoch 45/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0402 - acc: 0.9905 - val_loss: 0.1851 - val_acc: 0.4000\n",
      "Epoch 46/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0539 - acc: 0.9619 - val_loss: 0.1797 - val_acc: 0.4000\n",
      "Epoch 47/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0415 - acc: 0.9810 - val_loss: 0.1745 - val_acc: 0.4222\n",
      "Epoch 48/300\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0581 - acc: 0.9619 - val_loss: 0.1739 - val_acc: 0.4222\n",
      "Epoch 49/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0335 - acc: 0.9714 - val_loss: 0.1731 - val_acc: 0.4222\n",
      "Epoch 50/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0392 - acc: 0.9619 - val_loss: 0.1706 - val_acc: 0.5111\n",
      "Epoch 51/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0478 - acc: 0.9429 - val_loss: 0.1660 - val_acc: 0.6444\n",
      "Epoch 52/300\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0330 - acc: 0.9905 - val_loss: 0.1596 - val_acc: 0.6889\n",
      "Epoch 53/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0312 - acc: 0.9905 - val_loss: 0.1538 - val_acc: 0.6889\n",
      "Epoch 54/300\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0269 - acc: 1.0000 - val_loss: 0.1512 - val_acc: 0.7111\n",
      "Epoch 55/300\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0389 - acc: 0.9714 - val_loss: 0.1437 - val_acc: 0.7556\n",
      "Epoch 56/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0277 - acc: 0.9905 - val_loss: 0.1377 - val_acc: 0.7556\n",
      "Epoch 57/300\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0314 - acc: 0.9810 - val_loss: 0.1411 - val_acc: 0.7556\n",
      "Epoch 58/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0357 - acc: 0.9524 - val_loss: 0.1397 - val_acc: 0.7556\n",
      "Epoch 59/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0475 - acc: 0.9714 - val_loss: 0.1333 - val_acc: 0.7556\n",
      "Epoch 60/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0290 - acc: 1.0000 - val_loss: 0.1288 - val_acc: 0.7556\n",
      "Epoch 61/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 85us/step - loss: 0.0492 - acc: 0.9429 - val_loss: 0.1296 - val_acc: 0.7556\n",
      "Epoch 62/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0263 - acc: 0.9905 - val_loss: 0.1315 - val_acc: 0.7556\n",
      "Epoch 63/300\n",
      "105/105 [==============================] - 0s 96us/step - loss: 0.0422 - acc: 0.9810 - val_loss: 0.1237 - val_acc: 0.7556\n",
      "Epoch 64/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0392 - acc: 0.9905 - val_loss: 0.1220 - val_acc: 0.7556\n",
      "Epoch 65/300\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0393 - acc: 0.9810 - val_loss: 0.1230 - val_acc: 0.8222\n",
      "Epoch 66/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0389 - acc: 0.9714 - val_loss: 0.1197 - val_acc: 0.8222\n",
      "Epoch 67/300\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0327 - acc: 0.9905 - val_loss: 0.1218 - val_acc: 0.7778\n",
      "Epoch 68/300\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0347 - acc: 0.9810 - val_loss: 0.1183 - val_acc: 0.7556\n",
      "Epoch 69/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0356 - acc: 0.9524 - val_loss: 0.1115 - val_acc: 0.8222\n",
      "Epoch 70/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0302 - acc: 0.9810 - val_loss: 0.1108 - val_acc: 0.8444\n",
      "Epoch 71/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0274 - acc: 0.9810 - val_loss: 0.1095 - val_acc: 0.8667\n",
      "Epoch 72/300\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0284 - acc: 0.9905 - val_loss: 0.1037 - val_acc: 0.8667\n",
      "Epoch 73/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0380 - acc: 0.9524 - val_loss: 0.0989 - val_acc: 0.8667\n",
      "Epoch 74/300\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0215 - acc: 0.9905 - val_loss: 0.0956 - val_acc: 0.8667\n",
      "Epoch 75/300\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0277 - acc: 0.9714 - val_loss: 0.0916 - val_acc: 0.8667\n",
      "Epoch 76/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0248 - acc: 1.0000 - val_loss: 0.0935 - val_acc: 0.8667\n",
      "Epoch 77/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0336 - acc: 0.9714 - val_loss: 0.0951 - val_acc: 0.8667\n",
      "Epoch 78/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0264 - acc: 0.9905 - val_loss: 0.0894 - val_acc: 0.8444\n",
      "Epoch 79/300\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0328 - acc: 0.9714 - val_loss: 0.0839 - val_acc: 0.8444\n",
      "Epoch 80/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0286 - acc: 0.9810 - val_loss: 0.0867 - val_acc: 0.8444\n",
      "Epoch 81/300\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0279 - acc: 0.9810 - val_loss: 0.0851 - val_acc: 0.8667\n",
      "Epoch 82/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0318 - acc: 0.9810 - val_loss: 0.0807 - val_acc: 0.8667\n",
      "Epoch 83/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0253 - acc: 0.9905 - val_loss: 0.0826 - val_acc: 0.8667\n",
      "Epoch 84/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0353 - acc: 0.9619 - val_loss: 0.0810 - val_acc: 0.8667\n",
      "Epoch 85/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0208 - acc: 0.9810 - val_loss: 0.0768 - val_acc: 0.8889\n",
      "Epoch 86/300\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0303 - acc: 0.9905 - val_loss: 0.0779 - val_acc: 0.8889\n",
      "Epoch 87/300\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0281 - acc: 0.9810 - val_loss: 0.0769 - val_acc: 0.8889\n",
      "Epoch 88/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0261 - acc: 1.0000 - val_loss: 0.0711 - val_acc: 0.8889\n",
      "Epoch 89/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0239 - acc: 1.0000 - val_loss: 0.0683 - val_acc: 0.8889\n",
      "Epoch 90/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0262 - acc: 0.9905 - val_loss: 0.0692 - val_acc: 0.8889\n",
      "Epoch 91/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0261 - acc: 0.9905 - val_loss: 0.0700 - val_acc: 0.8889\n",
      "Epoch 92/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0195 - acc: 0.9905 - val_loss: 0.0678 - val_acc: 0.8889\n",
      "Epoch 93/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0228 - acc: 0.9905 - val_loss: 0.0621 - val_acc: 0.8889\n",
      "Epoch 94/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0239 - acc: 1.0000 - val_loss: 0.0606 - val_acc: 0.9111\n",
      "Epoch 95/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0281 - acc: 0.9905 - val_loss: 0.0588 - val_acc: 0.9111\n",
      "Epoch 96/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0358 - acc: 0.9714 - val_loss: 0.0579 - val_acc: 0.8889\n",
      "Epoch 97/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0300 - acc: 0.9810 - val_loss: 0.0625 - val_acc: 0.8889\n",
      "Epoch 98/300\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0367 - acc: 0.9905 - val_loss: 0.0648 - val_acc: 0.8889\n",
      "Epoch 99/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0245 - acc: 0.9905 - val_loss: 0.0603 - val_acc: 0.9111\n",
      "Epoch 100/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0275 - acc: 0.9905 - val_loss: 0.0552 - val_acc: 0.9111\n",
      "Epoch 101/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0392 - acc: 0.9714 - val_loss: 0.0546 - val_acc: 0.9333\n",
      "Epoch 102/300\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0220 - acc: 0.9905 - val_loss: 0.0535 - val_acc: 0.8889\n",
      "Epoch 103/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0465 - acc: 0.9524 - val_loss: 0.0556 - val_acc: 0.9111\n",
      "Epoch 104/300\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0340 - acc: 0.9619 - val_loss: 0.0684 - val_acc: 0.9556\n",
      "Epoch 105/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0360 - acc: 0.9810 - val_loss: 0.0643 - val_acc: 0.9778\n",
      "Epoch 106/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0278 - acc: 0.9714 - val_loss: 0.0563 - val_acc: 0.9556\n",
      "Epoch 107/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0283 - acc: 0.9905 - val_loss: 0.0529 - val_acc: 0.9556\n",
      "Epoch 108/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0226 - acc: 1.0000 - val_loss: 0.0492 - val_acc: 0.9556\n",
      "Epoch 109/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0225 - acc: 0.9905 - val_loss: 0.0533 - val_acc: 0.9556\n",
      "Epoch 110/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0245 - acc: 0.9714 - val_loss: 0.0586 - val_acc: 0.9556\n",
      "Epoch 111/300\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0292 - acc: 0.9905 - val_loss: 0.0561 - val_acc: 0.9556\n",
      "Epoch 112/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0217 - acc: 1.0000 - val_loss: 0.0462 - val_acc: 0.9556\n",
      "Epoch 113/300\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.0200 - acc: 1.000 - 0s 95us/step - loss: 0.0221 - acc: 0.9905 - val_loss: 0.0395 - val_acc: 0.9556\n",
      "Epoch 114/300\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0234 - acc: 0.9714 - val_loss: 0.0369 - val_acc: 0.9556\n",
      "Epoch 115/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0272 - acc: 0.9714 - val_loss: 0.0394 - val_acc: 0.9556\n",
      "Epoch 116/300\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0208 - acc: 1.0000 - val_loss: 0.0399 - val_acc: 0.9556\n",
      "Epoch 117/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0230 - acc: 0.9905 - val_loss: 0.0380 - val_acc: 0.9556\n",
      "Epoch 118/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0239 - acc: 0.9810 - val_loss: 0.0354 - val_acc: 0.9556\n",
      "Epoch 119/300\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0322 - acc: 0.9619 - val_loss: 0.0363 - val_acc: 0.9556\n",
      "Epoch 120/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0178 - acc: 1.0000 - val_loss: 0.0389 - val_acc: 0.9556\n",
      "Epoch 121/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 76us/step - loss: 0.0286 - acc: 0.9810 - val_loss: 0.0394 - val_acc: 0.9778\n",
      "Epoch 122/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0294 - acc: 0.9714 - val_loss: 0.0354 - val_acc: 0.9778\n",
      "Epoch 123/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0171 - acc: 0.9905 - val_loss: 0.0380 - val_acc: 0.9778\n",
      "Epoch 124/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0212 - acc: 0.9905 - val_loss: 0.0351 - val_acc: 0.9778\n",
      "Epoch 125/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0290 - acc: 0.9810 - val_loss: 0.0359 - val_acc: 0.9778\n",
      "Epoch 126/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0231 - acc: 0.9905 - val_loss: 0.0334 - val_acc: 0.9778\n",
      "Epoch 127/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0264 - acc: 0.9810 - val_loss: 0.0321 - val_acc: 0.9778\n",
      "Epoch 128/300\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0308 - acc: 0.9714 - val_loss: 0.0310 - val_acc: 0.9778\n",
      "Epoch 129/300\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0186 - acc: 1.0000 - val_loss: 0.0292 - val_acc: 0.9556\n",
      "Epoch 130/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0243 - acc: 0.9810 - val_loss: 0.0332 - val_acc: 0.9556\n",
      "Epoch 131/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0266 - acc: 0.9714 - val_loss: 0.0362 - val_acc: 0.9778\n",
      "Epoch 132/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0303 - acc: 0.9714 - val_loss: 0.0381 - val_acc: 0.9778\n",
      "Epoch 133/300\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0256 - acc: 0.9810 - val_loss: 0.0398 - val_acc: 0.9778\n",
      "Epoch 134/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0179 - acc: 0.9905 - val_loss: 0.0310 - val_acc: 0.9778\n",
      "Epoch 135/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0208 - acc: 0.9905 - val_loss: 0.0295 - val_acc: 0.9778\n",
      "Epoch 136/300\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0171 - acc: 1.0000 - val_loss: 0.0335 - val_acc: 0.9778\n",
      "Epoch 137/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0205 - acc: 0.9810 - val_loss: 0.0316 - val_acc: 0.9778\n",
      "Epoch 138/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0275 - acc: 0.9619 - val_loss: 0.0273 - val_acc: 0.9778\n",
      "Epoch 139/300\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0222 - acc: 0.9810 - val_loss: 0.0279 - val_acc: 0.9556\n",
      "Epoch 140/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0260 - acc: 0.9714 - val_loss: 0.0261 - val_acc: 0.9556\n",
      "Epoch 141/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0258 - acc: 0.9905 - val_loss: 0.0252 - val_acc: 0.9556\n",
      "Epoch 142/300\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0161 - acc: 1.0000 - val_loss: 0.0274 - val_acc: 0.9333\n",
      "Epoch 143/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0226 - acc: 1.0000 - val_loss: 0.0263 - val_acc: 0.9333\n",
      "Epoch 144/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0269 - acc: 0.9905 - val_loss: 0.0266 - val_acc: 0.9333\n",
      "Epoch 145/300\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0200 - acc: 1.0000 - val_loss: 0.0254 - val_acc: 0.9333\n",
      "Epoch 146/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0156 - acc: 1.0000 - val_loss: 0.0237 - val_acc: 0.9556\n",
      "Epoch 147/300\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0161 - acc: 1.0000 - val_loss: 0.0233 - val_acc: 0.9778\n",
      "Epoch 148/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0133 - acc: 1.0000 - val_loss: 0.0203 - val_acc: 0.9778\n",
      "Epoch 149/300\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.0156 - acc: 0.9905 - val_loss: 0.0204 - val_acc: 0.9778\n",
      "Epoch 150/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0180 - acc: 0.9810 - val_loss: 0.0226 - val_acc: 0.9778\n",
      "Epoch 151/300\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0162 - acc: 0.9905 - val_loss: 0.0175 - val_acc: 0.9778\n",
      "Epoch 152/300\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.0259 - acc: 0.9714 - val_loss: 0.0170 - val_acc: 0.9778\n",
      "Epoch 153/300\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.0188 - acc: 0.9905 - val_loss: 0.0189 - val_acc: 0.9556\n",
      "Epoch 154/300\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0116 - acc: 1.0000 - val_loss: 0.0181 - val_acc: 0.9778\n",
      "Epoch 155/300\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0176 - acc: 0.9905 - val_loss: 0.0182 - val_acc: 0.9556\n",
      "Epoch 156/300\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0161 - acc: 0.9810 - val_loss: 0.0206 - val_acc: 0.9778\n",
      "Epoch 157/300\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0151 - acc: 1.0000 - val_loss: 0.0207 - val_acc: 0.9556\n",
      "Epoch 158/300\n",
      "105/105 [==============================] - 0s 105us/step - loss: 0.0180 - acc: 0.9905 - val_loss: 0.0206 - val_acc: 0.9778\n",
      "Epoch 159/300\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0198 - acc: 0.9905 - val_loss: 0.0205 - val_acc: 0.9556\n",
      "Epoch 160/300\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0173 - acc: 0.9905 - val_loss: 0.0206 - val_acc: 0.9556\n",
      "Epoch 161/300\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0216 - acc: 1.0000 - val_loss: 0.0196 - val_acc: 0.9556\n",
      "Epoch 162/300\n",
      "105/105 [==============================] - 0s 95us/step - loss: 0.0149 - acc: 1.0000 - val_loss: 0.0178 - val_acc: 0.9556\n"
     ]
    }
   ],
   "source": [
    "# モデル生成\n",
    "model = Sequential()\n",
    "\n",
    "# 層の追加\n",
    "layers=[\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.01),\n",
    "    normalization.BatchNormalization(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.01),\n",
    "    normalization.BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(3, activation='linear')\n",
    "]\n",
    "for layer in layers:\n",
    "    model.add(layer)\n",
    "\n",
    "# モデルの学習設定\n",
    "\n",
    "model.compile(\n",
    "    loss=losses.mean_squared_error,\n",
    "    optimizer=optimizers.Adam(),\n",
    "    metrics=['acc']\n",
    ")\n",
    "\n",
    "# モデルの学習\n",
    "result = model.fit(\n",
    "    X_train_n,\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=300, \n",
    "    validation_data=(X_test_n,y_test),\n",
    "    callbacks=[\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10         # 様子見するエポック数\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.学習過程の保存（ModelCheckpoint）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 45 samples\n",
      "Epoch 1/10\n",
      "105/105 [==============================] - 0s 3ms/step - loss: 1.1029 - acc: 0.3619 - val_loss: 0.2991 - val_acc: 0.4222\n",
      "Epoch 2/10\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.3262 - acc: 0.7429 - val_loss: 0.2909 - val_acc: 0.2444\n",
      "Epoch 3/10\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.2937 - acc: 0.7810 - val_loss: 0.2904 - val_acc: 0.2444\n",
      "Epoch 4/10\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.2667 - acc: 0.8571 - val_loss: 0.2890 - val_acc: 0.2667\n",
      "Epoch 5/10\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.1801 - acc: 0.9048 - val_loss: 0.2847 - val_acc: 0.3111\n",
      "Epoch 6/10\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.1616 - acc: 0.8762 - val_loss: 0.2831 - val_acc: 0.3111\n",
      "Epoch 7/10\n",
      "105/105 [==============================] - 0s 85us/step - loss: 0.1589 - acc: 0.8857 - val_loss: 0.2864 - val_acc: 0.2889\n",
      "Epoch 8/10\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.1730 - acc: 0.8571 - val_loss: 0.2897 - val_acc: 0.2889\n",
      "Epoch 9/10\n",
      "105/105 [==============================] - 0s 86us/step - loss: 0.1041 - acc: 0.9429 - val_loss: 0.2933 - val_acc: 0.2667\n",
      "Epoch 10/10\n",
      "105/105 [==============================] - 0s 76us/step - loss: 0.1252 - acc: 0.9143 - val_loss: 0.2905 - val_acc: 0.2667\n"
     ]
    }
   ],
   "source": [
    "# モデル生成\n",
    "model = Sequential()\n",
    "\n",
    "# 層の追加\n",
    "layers=[\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.01),\n",
    "    normalization.BatchNormalization(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.01),\n",
    "    normalization.BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(3, activation='linear')\n",
    "]\n",
    "for layer in layers:\n",
    "    model.add(layer)\n",
    "\n",
    "# モデルの学習設定\n",
    "\n",
    "model.compile(\n",
    "    loss=losses.mean_squared_error,\n",
    "    optimizer=optimizers.Adam(),\n",
    "    metrics=['acc']\n",
    ")\n",
    "\n",
    "# モデルの学習\n",
    "result = model.fit(\n",
    "    X_train_n,\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_data=(X_test_n,y_test),\n",
    "    callbacks=[\n",
    "        callbacks.ModelCheckpoint(\n",
    "            filepath = './model/best_model.h5', # 最適なモデルの保存先\n",
    "            monitor='val_loss', \n",
    "            save_best_only=True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03573033  0.06635085  0.14308853]\n",
      " [-0.00327528  0.03158434  0.09970853]\n",
      " [ 0.09033719  0.02073036  0.13417807]\n",
      " [-0.0067401   0.06493506  0.13224591]\n",
      " [ 0.08129356  0.0442173   0.09734377]\n",
      " [ 0.03243033  0.07642076  0.15706444]\n",
      " [ 0.09244575  0.04680001  0.10830744]\n",
      " [ 0.02200171  0.05724183  0.11526815]\n",
      " [ 0.00589375  0.04798525  0.10817331]\n",
      " [ 0.02309287  0.05579779  0.10981441]\n",
      " [ 0.00479711  0.0508232   0.1264627 ]\n",
      " [ 0.03610555  0.06152822  0.12439085]\n",
      " [ 0.01572366  0.05868582  0.12060364]\n",
      " [ 0.01494378  0.05046656  0.10711181]\n",
      " [ 0.02788045  0.05864726  0.12536177]\n",
      " [ 0.08232848  0.03382356  0.11159534]\n",
      " [ 0.03564821  0.05656845  0.12353525]\n",
      " [ 0.02170213  0.05094036  0.12563652]\n",
      " [ 0.08035947  0.05505717  0.07133625]\n",
      " [ 0.10219454  0.04024743  0.12509441]\n",
      " [ 0.04416632  0.0576722   0.13768822]\n",
      " [ 0.0490251   0.06380091  0.142287  ]\n",
      " [ 0.09015812  0.03626287  0.09555189]\n",
      " [ 0.08705414  0.03880926  0.05473424]\n",
      " [ 0.03160701  0.0517583   0.11708224]\n",
      " [ 0.0971231   0.04305362  0.11881515]\n",
      " [ 0.10272531  0.04094046  0.1248172 ]\n",
      " [ 0.02480954  0.05935786  0.11415414]\n",
      " [ 0.03469298  0.04031488  0.10763676]\n",
      " [ 0.09628266  0.05329368  0.10968826]\n",
      " [ 0.02958766  0.06255433  0.1349558 ]\n",
      " [ 0.05350939  0.06375131  0.14643027]\n",
      " [ 0.07957776  0.04335756  0.10591276]\n",
      " [ 0.04029217  0.05611398  0.12916939]\n",
      " [ 0.02524992  0.07048775  0.13284905]\n",
      " [ 0.04899364  0.0505364   0.13214366]\n",
      " [ 0.08438264  0.03943089  0.11180916]\n",
      " [ 0.02429016  0.05198006  0.12366726]\n",
      " [ 0.04340823  0.05733279  0.12588176]\n",
      " [ 0.0189007   0.05288389  0.11396568]\n",
      " [ 0.01911516  0.06307635  0.12099023]\n",
      " [ 0.08352782  0.04662176  0.08116275]\n",
      " [ 0.03028922  0.06239489  0.10988057]\n",
      " [ 0.1107792   0.05372049  0.12824076]\n",
      " [ 0.07996941  0.03648284  0.11182246]]\n"
     ]
    }
   ],
   "source": [
    "# 最適モデルの読み込み\n",
    "model = keras.models.load_model('./model/best_model.h5')\n",
    "y_predict = model.predict(X_test_n)\n",
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自作のコールバック関数\n",
    "\n",
    "コールバック関数は、自作したものを使用することもできる。\n",
    "\n",
    "今回は、EPOCH毎に、精度を描画するコールバック関数を実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        # 各epoch毎にlossを格納していく\n",
    "        self.loss = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \"\"\"各epoch終了ごとに呼び出される\n",
    "        \"\"\"\n",
    "        # lossを格納\n",
    "        self.loss.append(logs['loss'])\n",
    "        \n",
    "        # グラフ初期化\n",
    "        clear_output(wait = True)\n",
    "        \n",
    "        # グラフ描画部\n",
    "        plt.figure(num=1, clear=True)\n",
    "        plt.title('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.plot(self.loss, label='train')\n",
    "        plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 必要ライブラリのインポート\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# モデル生成\n",
    "model = Sequential()\n",
    "\n",
    "# 層の追加\n",
    "layers=[\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.01),\n",
    "    normalization.BatchNormalization(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.01),\n",
    "    normalization.BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(3, activation='linear')\n",
    "]\n",
    "for layer in layers:\n",
    "    model.add(layer)\n",
    "\n",
    "# モデルの学習設定\n",
    "\n",
    "model.compile(\n",
    "    loss=losses.mean_squared_error,\n",
    "    optimizer=optimizers.Adam(),\n",
    "    metrics=['acc']\n",
    ")\n",
    "\n",
    "# モデルの学習\n",
    "result = model.fit(\n",
    "    X_train_n,\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    verbose=False,\n",
    "    validation_data=(X_test_n,y_test),\n",
    "    callbacks=[\n",
    "        LossHistory()\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
