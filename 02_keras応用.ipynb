{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kerasには下記のような機能があります。\n",
    "\n",
    "## ①学習の可視化\n",
    "## ②モデルの保存と読み込み\n",
    "## ③コールバック\n",
    "### 　　　1.学習過程の可視化\n",
    "### 　　　2.学習の早期終了\n",
    "## ④最適なモデルの選択"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 連続値データの読み込み\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "# 訓練データとテストデータに分ける\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris['data'], iris['target'], test_size=0.3,  random_state=0)\n",
    "\n",
    "\"\"\"\n",
    "# 標準化（Standardization）\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test) \n",
    "scaler = StandardScaler()\n",
    "y_train_s = scaler.fit_transform(y_train.reshape(len(y_train),1))\n",
    "y_test_s = scaler.transform(y_test.reshape(len(y_test),1)) \n",
    "\"\"\"\n",
    "\n",
    "# 正規化（Normarization）\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler_x = MinMaxScaler()\n",
    "X_train_n = scaler_x.fit_transform(X_train)\n",
    "X_test_n = scaler_x.transform(X_test) \n",
    "\n",
    "# one-hotベクトル化\n",
    "import keras\n",
    "y_train = keras.utils.to_categorical(y_train,3)\n",
    "y_test = keras.utils.to_categorical(y_test,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ①学習の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 必要ライブラリのインポート\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.layers import normalization\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "105/105 [==============================] - 0s 3ms/step - loss: 0.6354 - acc: 0.4952\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.3273 - acc: 0.7619\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - 0s 162us/step - loss: 0.2495 - acc: 0.8286\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.1954 - acc: 0.8762\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.1814 - acc: 0.8952\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.1512 - acc: 0.9429\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.1228 - acc: 0.9238\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.1017 - acc: 0.9143\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.1046 - acc: 0.9333\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0991 - acc: 0.9429\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0879 - acc: 0.9714\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.1001 - acc: 0.9333\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.1088 - acc: 0.8952\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0754 - acc: 0.9429\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - 0s 162us/step - loss: 0.0888 - acc: 0.9333\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0680 - acc: 0.9524\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0675 - acc: 0.9619\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0551 - acc: 0.9524\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0937 - acc: 0.9333\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0739 - acc: 0.9524\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0851 - acc: 0.9143\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0644 - acc: 0.9238\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0601 - acc: 0.9714\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0627 - acc: 0.9619\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - 0s 153us/step - loss: 0.0600 - acc: 0.9905\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0702 - acc: 0.9524\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0765 - acc: 0.9524\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0605 - acc: 0.9333\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0563 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0509 - acc: 0.9905\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0594 - acc: 0.9810\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0607 - acc: 0.9714\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0646 - acc: 0.9905\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0519 - acc: 0.9905\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0662 - acc: 0.9619\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - 0s 153us/step - loss: 0.0449 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0477 - acc: 0.9619\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0546 - acc: 0.9619\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - 0s 134us/step - loss: 0.0598 - acc: 0.9905\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0503 - acc: 0.9429\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0556 - acc: 0.9619\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - 0s 134us/step - loss: 0.0386 - acc: 0.9810\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0417 - acc: 0.9524\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0476 - acc: 0.9905\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0427 - acc: 0.9619\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - 0s 134us/step - loss: 0.0372 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - 0s 153us/step - loss: 0.0441 - acc: 0.9619\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0651 - acc: 0.9238\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0415 - acc: 0.9619\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - 0s 153us/step - loss: 0.0424 - acc: 0.9714\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0397 - acc: 0.9810\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - 0s 162us/step - loss: 0.0468 - acc: 0.9619\n",
      "Epoch 53/100\n",
      "105/105 [==============================] - 0s 153us/step - loss: 0.0408 - acc: 0.9714\n",
      "Epoch 54/100\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0427 - acc: 0.9810\n",
      "Epoch 55/100\n",
      "105/105 [==============================] - 0s 134us/step - loss: 0.0355 - acc: 0.9810\n",
      "Epoch 56/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0371 - acc: 0.9905\n",
      "Epoch 57/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0349 - acc: 0.9905\n",
      "Epoch 58/100\n",
      "105/105 [==============================] - 0s 134us/step - loss: 0.0267 - acc: 0.9905\n",
      "Epoch 59/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0326 - acc: 0.9810\n",
      "Epoch 60/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0391 - acc: 0.9619\n",
      "Epoch 61/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0331 - acc: 0.9905\n",
      "Epoch 62/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0350 - acc: 0.9714\n",
      "Epoch 63/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0298 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0435 - acc: 0.9619\n",
      "Epoch 65/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0301 - acc: 0.9905\n",
      "Epoch 66/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0285 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0282 - acc: 0.9905\n",
      "Epoch 68/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0352 - acc: 0.9619\n",
      "Epoch 69/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0343 - acc: 0.9714\n",
      "Epoch 70/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0288 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0431 - acc: 0.9619\n",
      "Epoch 72/100\n",
      "105/105 [==============================] - 0s 134us/step - loss: 0.0353 - acc: 0.9905\n",
      "Epoch 73/100\n",
      "105/105 [==============================] - 0s 153us/step - loss: 0.0384 - acc: 0.9714\n",
      "Epoch 74/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0259 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0291 - acc: 0.9905\n",
      "Epoch 76/100\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0310 - acc: 0.9810\n",
      "Epoch 77/100\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0298 - acc: 0.9905\n",
      "Epoch 78/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0358 - acc: 0.9810\n",
      "Epoch 79/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0236 - acc: 0.9905\n",
      "Epoch 80/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0261 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0232 - acc: 0.9905\n",
      "Epoch 82/100\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0332 - acc: 0.9905\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 143us/step - loss: 0.0241 - acc: 0.9905\n",
      "Epoch 84/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0276 - acc: 0.9810\n",
      "Epoch 85/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0228 - acc: 0.9905\n",
      "Epoch 86/100\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0358 - acc: 0.9810\n",
      "Epoch 87/100\n",
      "105/105 [==============================] - 0s 134us/step - loss: 0.0309 - acc: 0.9905\n",
      "Epoch 88/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0341 - acc: 0.9810\n",
      "Epoch 89/100\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0280 - acc: 0.9714\n",
      "Epoch 90/100\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0278 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0362 - acc: 0.9619\n",
      "Epoch 92/100\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0216 - acc: 0.9905\n",
      "Epoch 93/100\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0306 - acc: 0.9810\n",
      "Epoch 94/100\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0325 - acc: 0.9714\n",
      "Epoch 95/100\n",
      "105/105 [==============================] - 0s 134us/step - loss: 0.0274 - acc: 0.9905\n",
      "Epoch 96/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0236 - acc: 0.9905\n",
      "Epoch 97/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0216 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0210 - acc: 0.9905\n",
      "Epoch 99/100\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0321 - acc: 0.9619\n",
      "Epoch 100/100\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0310 - acc: 0.9714\n"
     ]
    }
   ],
   "source": [
    "# モデル生成\n",
    "model = Sequential()\n",
    "\n",
    "# 層の追加\n",
    "layers=[\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.01),\n",
    "    normalization.BatchNormalization(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.01),\n",
    "    normalization.BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(3, activation='linear')\n",
    "]\n",
    "for layer in layers:\n",
    "    model.add(layer)\n",
    "\n",
    "# モデルの学習設定\n",
    "model.compile(\n",
    "    loss=losses.mean_squared_error,\n",
    "    optimizer=optimizers.Adam(),\n",
    "    metrics=['acc']\n",
    ")\n",
    "\n",
    "# モデルの学習\n",
    "result = model.fit(\n",
    "    X_train_n,\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 描画データ\n",
    "acc = result.history['acc']  # 学習データの正答率\n",
    "loss = result.history['loss']  # 学習データの損失\n",
    "\n",
    "# x軸\n",
    "epochs = range(len(acc))\n",
    "\n",
    "# 図示\n",
    "plt.plot(epochs, acc, label='Train Accuracy')\n",
    "plt.plot(epochs, loss, label='Train Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ②モデルの保存と読み込み\n",
    "\n",
    "学習したモデルは、保存できます。\n",
    "\n",
    "保存したモデルは、使用したいときにプログラムから読み込み利用できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# モデルの保存\n",
    "model.save('./model/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# モデルの読み込み\n",
    "model = keras.models.load_model('./model/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 1 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 2 1 1 2 0 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "# 予測値の算出\n",
    "import numpy as np\n",
    "y_predict = np.argmax(model.predict(X_test_n),axis=1)\n",
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ③コールバック\n",
    "\n",
    "コールバックは訓練中で適用される関数のことです。訓練中にモデル内部の状態と統計量を可視化する際に、コールバックを使います。その他にも、各エポック毎に「何かしらの処理」を行いたい場合、ここに記述します。\n",
    "\n",
    "\n",
    "> コールバックについて紹介する前に、データセットの種類について紹介します。\n",
    "> \n",
    "> + **訓練データ**：重み更新に使用する。\n",
    "> + **テストデータ**：モデル性能の検証に利用する。\n",
    "> + **検証データ(バリデーションデータ)**：学習の早期終了（EarlyStopping）やハイパーパラメータの調整に利用する\n",
    "> \n",
    "> ＊ ほとんどの場合、ハイパーパラメータの調整は、訓練データを用いて行うため、気にしなくてよいが、学術的なデータ解析の際には、検証用データも使用する。ほとんどの実務的な場合においては、検証データは、学習の早期終了などに用いられると考えてよい。\n",
    "\n",
    "\n",
    "### コールバックで出来ること\n",
    "\n",
    "**1. 学習の早期終了（EarlyStopping）**：バリデーションデータへの精度に基づいて、改善しなかった場合は、学習を早期に打ち切り、過学習の防止に務める。\n",
    "\n",
    "**2. 学習過程の保存（ModelCheckpoint）**：学習の途中経過を記録しておく。今まで見てきた手法では、学習終了時点のモデルを採用し、予測値の算出を行ったが、学習途中の精度が良い場合がある。そのような場合は、このModelCheckpointで記録しておいたモデルの中から、最も精度のいいモデルを採用する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.学習の早期終了（EarlyStopping）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 45 samples\n",
      "Epoch 1/300\n",
      "105/105 [==============================] - 0s 4ms/step - loss: 1.3164 - acc: 0.4381 - val_loss: 0.3284 - val_acc: 0.2444\n",
      "Epoch 2/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.4150 - acc: 0.8762 - val_loss: 0.3019 - val_acc: 0.4667\n",
      "Epoch 3/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.2335 - acc: 0.8476 - val_loss: 0.2932 - val_acc: 0.4000\n",
      "Epoch 4/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.2317 - acc: 0.7905 - val_loss: 0.2883 - val_acc: 0.4000\n",
      "Epoch 5/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.2111 - acc: 0.8190 - val_loss: 0.2848 - val_acc: 0.4000\n",
      "Epoch 6/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.1788 - acc: 0.8762 - val_loss: 0.2812 - val_acc: 0.4000\n",
      "Epoch 7/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.1355 - acc: 0.8952 - val_loss: 0.2800 - val_acc: 0.4000\n",
      "Epoch 8/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.1349 - acc: 0.8857 - val_loss: 0.2785 - val_acc: 0.4000\n",
      "Epoch 9/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.1169 - acc: 0.9238 - val_loss: 0.2761 - val_acc: 0.4000\n",
      "Epoch 10/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.1328 - acc: 0.9238 - val_loss: 0.2745 - val_acc: 0.4000\n",
      "Epoch 11/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.1225 - acc: 0.9238 - val_loss: 0.2731 - val_acc: 0.4000\n",
      "Epoch 12/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0913 - acc: 0.9524 - val_loss: 0.2694 - val_acc: 0.4000\n",
      "Epoch 13/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0717 - acc: 0.9524 - val_loss: 0.2661 - val_acc: 0.4000\n",
      "Epoch 14/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0847 - acc: 0.9429 - val_loss: 0.2642 - val_acc: 0.4000\n",
      "Epoch 15/300\n",
      "105/105 [==============================] - 0s 191us/step - loss: 0.1080 - acc: 0.9238 - val_loss: 0.2615 - val_acc: 0.4222\n",
      "Epoch 16/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0916 - acc: 0.9524 - val_loss: 0.2574 - val_acc: 0.4222\n",
      "Epoch 17/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0908 - acc: 0.9619 - val_loss: 0.2544 - val_acc: 0.4222\n",
      "Epoch 18/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0837 - acc: 0.9524 - val_loss: 0.2521 - val_acc: 0.4444\n",
      "Epoch 19/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0769 - acc: 0.9429 - val_loss: 0.2495 - val_acc: 0.4889\n",
      "Epoch 20/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0684 - acc: 0.9524 - val_loss: 0.2445 - val_acc: 0.4667\n",
      "Epoch 21/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0671 - acc: 0.9905 - val_loss: 0.2393 - val_acc: 0.4667\n",
      "Epoch 22/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0674 - acc: 0.9714 - val_loss: 0.2366 - val_acc: 0.4667\n",
      "Epoch 23/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0768 - acc: 0.9429 - val_loss: 0.2342 - val_acc: 0.4889\n",
      "Epoch 24/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0714 - acc: 0.9524 - val_loss: 0.2318 - val_acc: 0.5111\n",
      "Epoch 25/300\n",
      "105/105 [==============================] - 0s 200us/step - loss: 0.0566 - acc: 0.9714 - val_loss: 0.2303 - val_acc: 0.5111\n",
      "Epoch 26/300\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.0811 - acc: 1.000 - 0s 190us/step - loss: 0.0681 - acc: 0.9524 - val_loss: 0.2286 - val_acc: 0.4667\n",
      "Epoch 27/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0541 - acc: 0.9905 - val_loss: 0.2274 - val_acc: 0.4667\n",
      "Epoch 28/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0654 - acc: 0.9429 - val_loss: 0.2226 - val_acc: 0.4667\n",
      "Epoch 29/300\n",
      "105/105 [==============================] - 0s 200us/step - loss: 0.0531 - acc: 0.9619 - val_loss: 0.2190 - val_acc: 0.4667\n",
      "Epoch 30/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0491 - acc: 0.9619 - val_loss: 0.2177 - val_acc: 0.4889\n",
      "Epoch 31/300\n",
      "105/105 [==============================] - 0s 200us/step - loss: 0.0404 - acc: 1.0000 - val_loss: 0.2187 - val_acc: 0.4667\n",
      "Epoch 32/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0551 - acc: 0.9714 - val_loss: 0.2178 - val_acc: 0.4889\n",
      "Epoch 33/300\n",
      "105/105 [==============================] - 0s 191us/step - loss: 0.0418 - acc: 0.9905 - val_loss: 0.2156 - val_acc: 0.4889\n",
      "Epoch 34/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0496 - acc: 0.9619 - val_loss: 0.2117 - val_acc: 0.5333\n",
      "Epoch 35/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0467 - acc: 0.9905 - val_loss: 0.2083 - val_acc: 0.5556\n",
      "Epoch 36/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0571 - acc: 0.9714 - val_loss: 0.2056 - val_acc: 0.5556\n",
      "Epoch 37/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0633 - acc: 0.9619 - val_loss: 0.2044 - val_acc: 0.5333\n",
      "Epoch 38/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0599 - acc: 0.9429 - val_loss: 0.2000 - val_acc: 0.5333\n",
      "Epoch 39/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0623 - acc: 0.9524 - val_loss: 0.1987 - val_acc: 0.5778\n",
      "Epoch 40/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0537 - acc: 0.9810 - val_loss: 0.2010 - val_acc: 0.5778\n",
      "Epoch 41/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0551 - acc: 0.9714 - val_loss: 0.2018 - val_acc: 0.5778\n",
      "Epoch 42/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0475 - acc: 1.0000 - val_loss: 0.2021 - val_acc: 0.5333\n",
      "Epoch 43/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0579 - acc: 0.9524 - val_loss: 0.2037 - val_acc: 0.5333\n",
      "Epoch 44/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0444 - acc: 0.9810 - val_loss: 0.2022 - val_acc: 0.5556\n",
      "Epoch 45/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0441 - acc: 0.9714 - val_loss: 0.1938 - val_acc: 0.5778\n",
      "Epoch 46/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0536 - acc: 0.9905 - val_loss: 0.1894 - val_acc: 0.5333\n",
      "Epoch 47/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0352 - acc: 0.9905 - val_loss: 0.1883 - val_acc: 0.6000\n",
      "Epoch 48/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0539 - acc: 0.9524 - val_loss: 0.1822 - val_acc: 0.6444\n",
      "Epoch 49/300\n",
      "105/105 [==============================] - 0s 200us/step - loss: 0.0418 - acc: 0.9810 - val_loss: 0.1813 - val_acc: 0.6444\n",
      "Epoch 50/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0403 - acc: 0.9810 - val_loss: 0.1814 - val_acc: 0.6667\n",
      "Epoch 51/300\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.0356 - acc: 0.968 - 0s 171us/step - loss: 0.0377 - acc: 0.9714 - val_loss: 0.1792 - val_acc: 0.6667\n",
      "Epoch 52/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0427 - acc: 0.9524 - val_loss: 0.1769 - val_acc: 0.6889\n",
      "Epoch 53/300\n",
      "105/105 [==============================] - 0s 200us/step - loss: 0.0277 - acc: 0.9905 - val_loss: 0.1760 - val_acc: 0.6667\n",
      "Epoch 54/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0369 - acc: 0.9810 - val_loss: 0.1733 - val_acc: 0.6222\n",
      "Epoch 55/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0296 - acc: 0.9905 - val_loss: 0.1678 - val_acc: 0.7556\n",
      "Epoch 56/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0342 - acc: 1.0000 - val_loss: 0.1661 - val_acc: 0.7556\n",
      "Epoch 57/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0337 - acc: 0.9714 - val_loss: 0.1625 - val_acc: 0.6889\n",
      "Epoch 58/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0307 - acc: 1.0000 - val_loss: 0.1579 - val_acc: 0.8000\n",
      "Epoch 59/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0481 - acc: 0.9429 - val_loss: 0.1560 - val_acc: 0.9111\n",
      "Epoch 60/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 181us/step - loss: 0.0421 - acc: 0.9810 - val_loss: 0.1577 - val_acc: 0.9556\n",
      "Epoch 61/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0434 - acc: 0.9810 - val_loss: 0.1539 - val_acc: 0.9111\n",
      "Epoch 62/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0375 - acc: 1.0000 - val_loss: 0.1475 - val_acc: 0.8889\n",
      "Epoch 63/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0342 - acc: 0.9810 - val_loss: 0.1426 - val_acc: 0.8667\n",
      "Epoch 64/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0355 - acc: 0.9905 - val_loss: 0.1437 - val_acc: 0.9333\n",
      "Epoch 65/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0444 - acc: 0.9810 - val_loss: 0.1423 - val_acc: 0.9778\n",
      "Epoch 66/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0324 - acc: 1.0000 - val_loss: 0.1438 - val_acc: 0.9556\n",
      "Epoch 67/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0446 - acc: 0.9619 - val_loss: 0.1391 - val_acc: 0.9556\n",
      "Epoch 68/300\n",
      "105/105 [==============================] - 0s 200us/step - loss: 0.0398 - acc: 0.9810 - val_loss: 0.1358 - val_acc: 0.9778\n",
      "Epoch 69/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0387 - acc: 0.9905 - val_loss: 0.1314 - val_acc: 0.9778\n",
      "Epoch 70/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0285 - acc: 0.9905 - val_loss: 0.1210 - val_acc: 0.9778\n",
      "Epoch 71/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0335 - acc: 0.9905 - val_loss: 0.1205 - val_acc: 0.9778\n",
      "Epoch 72/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0336 - acc: 0.9810 - val_loss: 0.1177 - val_acc: 0.9778\n",
      "Epoch 73/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0309 - acc: 0.9810 - val_loss: 0.1111 - val_acc: 0.9778\n",
      "Epoch 74/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0282 - acc: 0.9905 - val_loss: 0.1109 - val_acc: 0.9778\n",
      "Epoch 75/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0303 - acc: 0.9905 - val_loss: 0.1063 - val_acc: 0.9778\n",
      "Epoch 76/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0301 - acc: 0.9905 - val_loss: 0.1038 - val_acc: 0.9778\n",
      "Epoch 77/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0311 - acc: 0.9714 - val_loss: 0.1046 - val_acc: 0.9778\n",
      "Epoch 78/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0300 - acc: 0.9905 - val_loss: 0.1067 - val_acc: 0.9778\n",
      "Epoch 79/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0349 - acc: 0.9810 - val_loss: 0.1020 - val_acc: 0.9778\n",
      "Epoch 80/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0341 - acc: 0.9810 - val_loss: 0.0995 - val_acc: 0.9778\n",
      "Epoch 81/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0326 - acc: 0.9905 - val_loss: 0.0969 - val_acc: 0.9778\n",
      "Epoch 82/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0391 - acc: 0.9714 - val_loss: 0.0845 - val_acc: 0.9556\n",
      "Epoch 83/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0353 - acc: 0.9810 - val_loss: 0.0808 - val_acc: 0.9778\n",
      "Epoch 84/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0290 - acc: 0.9905 - val_loss: 0.0778 - val_acc: 0.9556\n",
      "Epoch 85/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0250 - acc: 0.9905 - val_loss: 0.0820 - val_acc: 0.9778\n",
      "Epoch 86/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0245 - acc: 0.9905 - val_loss: 0.0829 - val_acc: 0.9778\n",
      "Epoch 87/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0263 - acc: 0.9905 - val_loss: 0.0831 - val_acc: 0.9778\n",
      "Epoch 88/300\n",
      "105/105 [==============================] - 0s 172us/step - loss: 0.0340 - acc: 0.9714 - val_loss: 0.0807 - val_acc: 0.9778\n",
      "Epoch 89/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0405 - acc: 0.9810 - val_loss: 0.0789 - val_acc: 0.9778\n",
      "Epoch 90/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0335 - acc: 0.9810 - val_loss: 0.0745 - val_acc: 0.9778\n",
      "Epoch 91/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0272 - acc: 0.9905 - val_loss: 0.0724 - val_acc: 0.9778\n",
      "Epoch 92/300\n",
      "105/105 [==============================] - 0s 200us/step - loss: 0.0244 - acc: 0.9905 - val_loss: 0.0695 - val_acc: 0.9778\n",
      "Epoch 93/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0212 - acc: 0.9905 - val_loss: 0.0699 - val_acc: 0.9778\n",
      "Epoch 94/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0295 - acc: 0.9810 - val_loss: 0.0687 - val_acc: 0.9556\n",
      "Epoch 95/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0342 - acc: 0.9810 - val_loss: 0.0716 - val_acc: 0.9778\n",
      "Epoch 96/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0239 - acc: 1.0000 - val_loss: 0.0709 - val_acc: 0.9778\n",
      "Epoch 97/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0302 - acc: 0.9810 - val_loss: 0.0709 - val_acc: 0.9778\n",
      "Epoch 98/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0314 - acc: 0.9905 - val_loss: 0.0648 - val_acc: 0.9556\n",
      "Epoch 99/300\n",
      "105/105 [==============================] - 0s 200us/step - loss: 0.0202 - acc: 1.0000 - val_loss: 0.0608 - val_acc: 0.9556\n",
      "Epoch 100/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0198 - acc: 0.9905 - val_loss: 0.0595 - val_acc: 0.9778\n",
      "Epoch 101/300\n",
      "105/105 [==============================] - 0s 200us/step - loss: 0.0288 - acc: 0.9905 - val_loss: 0.0569 - val_acc: 0.9778\n",
      "Epoch 102/300\n",
      "105/105 [==============================] - 0s 200us/step - loss: 0.0285 - acc: 0.9905 - val_loss: 0.0612 - val_acc: 0.9778\n",
      "Epoch 103/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0315 - acc: 0.9905 - val_loss: 0.0552 - val_acc: 0.9778\n",
      "Epoch 104/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0234 - acc: 1.0000 - val_loss: 0.0568 - val_acc: 0.9778\n",
      "Epoch 105/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0222 - acc: 1.0000 - val_loss: 0.0610 - val_acc: 0.9778\n",
      "Epoch 106/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0325 - acc: 0.9810 - val_loss: 0.0572 - val_acc: 0.9778\n",
      "Epoch 107/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0273 - acc: 1.0000 - val_loss: 0.0520 - val_acc: 0.9778\n",
      "Epoch 108/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0313 - acc: 0.9619 - val_loss: 0.0497 - val_acc: 0.9778\n",
      "Epoch 109/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0255 - acc: 0.9905 - val_loss: 0.0513 - val_acc: 0.9778\n",
      "Epoch 110/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0250 - acc: 0.9905 - val_loss: 0.0502 - val_acc: 0.9778\n",
      "Epoch 111/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0276 - acc: 0.9905 - val_loss: 0.0424 - val_acc: 0.9778\n",
      "Epoch 112/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0204 - acc: 1.0000 - val_loss: 0.0431 - val_acc: 0.9778\n",
      "Epoch 113/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0282 - acc: 0.9905 - val_loss: 0.0423 - val_acc: 0.9778\n",
      "Epoch 114/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0187 - acc: 1.0000 - val_loss: 0.0454 - val_acc: 0.9778\n",
      "Epoch 115/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0262 - acc: 0.9714 - val_loss: 0.0454 - val_acc: 0.9778\n",
      "Epoch 116/300\n",
      "105/105 [==============================] - 0s 172us/step - loss: 0.0175 - acc: 1.0000 - val_loss: 0.0411 - val_acc: 0.9778\n",
      "Epoch 117/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0256 - acc: 1.0000 - val_loss: 0.0368 - val_acc: 0.9778\n",
      "Epoch 118/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0240 - acc: 1.0000 - val_loss: 0.0377 - val_acc: 0.9778\n",
      "Epoch 119/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0258 - acc: 0.9810 - val_loss: 0.0398 - val_acc: 0.9778\n",
      "Epoch 120/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 181us/step - loss: 0.0267 - acc: 0.9810 - val_loss: 0.0398 - val_acc: 0.9778\n",
      "Epoch 121/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0231 - acc: 0.9810 - val_loss: 0.0376 - val_acc: 0.9778\n",
      "Epoch 122/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0205 - acc: 1.0000 - val_loss: 0.0354 - val_acc: 0.9778\n",
      "Epoch 123/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0250 - acc: 0.9810 - val_loss: 0.0352 - val_acc: 0.9778\n",
      "Epoch 124/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0233 - acc: 0.9905 - val_loss: 0.0412 - val_acc: 0.9778\n",
      "Epoch 125/300\n",
      "105/105 [==============================] - 0s 200us/step - loss: 0.0228 - acc: 0.9810 - val_loss: 0.0383 - val_acc: 0.9778\n",
      "Epoch 126/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0306 - acc: 0.9810 - val_loss: 0.0337 - val_acc: 0.9778\n",
      "Epoch 127/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0248 - acc: 1.0000 - val_loss: 0.0332 - val_acc: 0.9778\n",
      "Epoch 128/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0256 - acc: 0.9810 - val_loss: 0.0355 - val_acc: 0.9778\n",
      "Epoch 129/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0245 - acc: 0.9905 - val_loss: 0.0336 - val_acc: 0.9778\n",
      "Epoch 130/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0313 - acc: 0.9810 - val_loss: 0.0327 - val_acc: 0.9778\n",
      "Epoch 131/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0234 - acc: 0.9905 - val_loss: 0.0344 - val_acc: 0.9778\n",
      "Epoch 132/300\n",
      "105/105 [==============================] - 0s 172us/step - loss: 0.0166 - acc: 1.0000 - val_loss: 0.0366 - val_acc: 0.9778\n",
      "Epoch 133/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0208 - acc: 0.9905 - val_loss: 0.0339 - val_acc: 0.9778\n",
      "Epoch 134/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0290 - acc: 0.9810 - val_loss: 0.0317 - val_acc: 0.9778\n",
      "Epoch 135/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0315 - acc: 0.9714 - val_loss: 0.0306 - val_acc: 0.9778\n",
      "Epoch 136/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0249 - acc: 0.9905 - val_loss: 0.0285 - val_acc: 0.9778\n",
      "Epoch 137/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0266 - acc: 0.9810 - val_loss: 0.0236 - val_acc: 0.9778\n",
      "Epoch 138/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0248 - acc: 1.0000 - val_loss: 0.0294 - val_acc: 0.9778\n",
      "Epoch 139/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0220 - acc: 1.0000 - val_loss: 0.0297 - val_acc: 0.9778\n",
      "Epoch 140/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0253 - acc: 0.9810 - val_loss: 0.0349 - val_acc: 0.9778\n",
      "Epoch 141/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0219 - acc: 0.9905 - val_loss: 0.0324 - val_acc: 0.9778\n",
      "Epoch 142/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 0.0241 - val_acc: 0.9778\n",
      "Epoch 143/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0238 - acc: 0.9905 - val_loss: 0.0225 - val_acc: 0.9778\n",
      "Epoch 144/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0294 - acc: 0.9714 - val_loss: 0.0225 - val_acc: 0.9778\n",
      "Epoch 145/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0230 - acc: 1.0000 - val_loss: 0.0216 - val_acc: 0.9778\n",
      "Epoch 146/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0229 - acc: 0.9905 - val_loss: 0.0186 - val_acc: 0.9778\n",
      "Epoch 147/300\n",
      "105/105 [==============================] - 0s 200us/step - loss: 0.0214 - acc: 0.9905 - val_loss: 0.0261 - val_acc: 0.9778\n",
      "Epoch 148/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0265 - acc: 0.9810 - val_loss: 0.0198 - val_acc: 0.9778\n",
      "Epoch 149/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 0.0215 - val_acc: 0.9778\n",
      "Epoch 150/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0211 - acc: 0.9810 - val_loss: 0.0278 - val_acc: 0.9778\n",
      "Epoch 151/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0286 - acc: 0.9905 - val_loss: 0.0277 - val_acc: 0.9778\n",
      "Epoch 152/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0232 - acc: 0.9905 - val_loss: 0.0229 - val_acc: 0.9778\n",
      "Epoch 153/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0287 - acc: 0.9810 - val_loss: 0.0215 - val_acc: 0.9778\n",
      "Epoch 154/300\n",
      "105/105 [==============================] - 0s 191us/step - loss: 0.0164 - acc: 0.9905 - val_loss: 0.0262 - val_acc: 0.9778\n",
      "Epoch 155/300\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0190 - acc: 0.9905 - val_loss: 0.0238 - val_acc: 0.9778\n",
      "Epoch 156/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0186 - acc: 0.9810 - val_loss: 0.0196 - val_acc: 0.9778\n"
     ]
    }
   ],
   "source": [
    "# モデル生成\n",
    "model = Sequential()\n",
    "\n",
    "# 層の追加\n",
    "layers=[\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.01),\n",
    "    normalization.BatchNormalization(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.01),\n",
    "    normalization.BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(3, activation='linear')\n",
    "]\n",
    "for layer in layers:\n",
    "    model.add(layer)\n",
    "\n",
    "# モデルの学習設定\n",
    "\n",
    "model.compile(\n",
    "    loss=losses.mean_squared_error,\n",
    "    optimizer=optimizers.Adam(),\n",
    "    metrics=['acc']\n",
    ")\n",
    "\n",
    "# モデルの学習\n",
    "result = model.fit(\n",
    "    X_train_n,\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=300, \n",
    "    validation_data=(X_test_n,y_test),\n",
    "    callbacks=[\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10         # 様子見するエポック数\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.学習過程の保存（ModelCheckpoint）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 45 samples\n",
      "Epoch 1/10\n",
      "105/105 [==============================] - 0s 4ms/step - loss: 0.7074 - acc: 0.5905 - val_loss: 0.3233 - val_acc: 0.2889\n",
      "Epoch 2/10\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.3371 - acc: 0.7524 - val_loss: 0.3232 - val_acc: 0.2444\n",
      "Epoch 3/10\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.2432 - acc: 0.8762 - val_loss: 0.3224 - val_acc: 0.2444\n",
      "Epoch 4/10\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.2093 - acc: 0.8571 - val_loss: 0.3241 - val_acc: 0.2444\n",
      "Epoch 5/10\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.1684 - acc: 0.8571 - val_loss: 0.3237 - val_acc: 0.2444\n",
      "Epoch 6/10\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.1614 - acc: 0.9143 - val_loss: 0.3197 - val_acc: 0.2444\n",
      "Epoch 7/10\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.1286 - acc: 0.9143 - val_loss: 0.3147 - val_acc: 0.2444\n",
      "Epoch 8/10\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.1184 - acc: 0.9048 - val_loss: 0.3106 - val_acc: 0.2444\n",
      "Epoch 9/10\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.1016 - acc: 0.9333 - val_loss: 0.3067 - val_acc: 0.2444\n",
      "Epoch 10/10\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.1254 - acc: 0.8571 - val_loss: 0.3037 - val_acc: 0.2444\n"
     ]
    }
   ],
   "source": [
    "# モデル生成\n",
    "model = Sequential()\n",
    "\n",
    "# 層の追加\n",
    "layers=[\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.01),\n",
    "    normalization.BatchNormalization(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.01),\n",
    "    normalization.BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(3, activation='linear')\n",
    "]\n",
    "for layer in layers:\n",
    "    model.add(layer)\n",
    "\n",
    "# モデルの学習設定\n",
    "\n",
    "model.compile(\n",
    "    loss=losses.mean_squared_error,\n",
    "    optimizer=optimizers.Adam(),\n",
    "    metrics=['acc']\n",
    ")\n",
    "\n",
    "# モデルの学習\n",
    "result = model.fit(\n",
    "    X_train_n,\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_data=(X_test_n,y_test),\n",
    "    callbacks=[\n",
    "        callbacks.ModelCheckpoint(\n",
    "            filepath = './model/best_model.h5', # 最適なモデルの保存先\n",
    "            monitor='val_loss', \n",
    "            save_best_only=True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00440958  0.04580278  0.18011513]\n",
      " [ 0.01435338  0.05232003  0.11687224]\n",
      " [-0.05130333  0.07952189  0.0949942 ]\n",
      " [ 0.01283246  0.02130981  0.15301771]\n",
      " [-0.01753366  0.10968068  0.07632756]\n",
      " [-0.0110977   0.04230192  0.17532042]\n",
      " [-0.01392779  0.10219075  0.08138037]\n",
      " [ 0.00215234  0.05905518  0.12589519]\n",
      " [ 0.01362194  0.04029968  0.13666159]\n",
      " [ 0.00126547  0.0657289   0.11558847]\n",
      " [ 0.01560441  0.00823521  0.15122259]\n",
      " [-0.00649495  0.06031531  0.12316133]\n",
      " [ 0.00736258  0.03562107  0.12608314]\n",
      " [ 0.00931341  0.04309843  0.13241078]\n",
      " [-0.00062125  0.03899299  0.12836364]\n",
      " [-0.02572781  0.11018369  0.07611452]\n",
      " [-0.00245537  0.0478132   0.12817799]\n",
      " [ 0.0097284   0.05378256  0.12954675]\n",
      " [ 0.01327489  0.11811356  0.06545831]\n",
      " [-0.02981246  0.07689313  0.09458838]\n",
      " [-0.00308196  0.04871584  0.16380206]\n",
      " [-0.01301078  0.05603161  0.13267119]\n",
      " [-0.01203427  0.11571261  0.07236993]\n",
      " [ 0.01642353  0.13232587  0.05172222]\n",
      " [ 0.00160113  0.03776541  0.14669459]\n",
      " [-0.01329374  0.10841316  0.07273075]\n",
      " [-0.02418587  0.08960436  0.08719267]\n",
      " [-0.00115918  0.05717545  0.11911123]\n",
      " [ 0.02599799  0.09752247  0.10734345]\n",
      " [-0.0038097   0.10332249  0.08343121]\n",
      " [-0.00802541  0.03224144  0.1422655 ]\n",
      " [-0.01263954  0.06524892  0.13705927]\n",
      " [-0.02647309  0.10562712  0.07992506]\n",
      " [-0.00525981  0.04388468  0.14240153]\n",
      " [ 0.00672305  0.02614471  0.16407347]\n",
      " [ 0.00169139  0.07794818  0.13676222]\n",
      " [-0.0406964   0.08448074  0.08941968]\n",
      " [ 0.00980827  0.02299831  0.14857545]\n",
      " [-0.01109129  0.06341814  0.1226856 ]\n",
      " [ 0.01240411  0.05765374  0.11973386]\n",
      " [ 0.00230382  0.03889005  0.15231353]\n",
      " [ 0.00040369  0.11511857  0.06587388]\n",
      " [-0.00360456  0.05664265  0.16106814]\n",
      " [ 0.00462063  0.09187993  0.09004647]\n",
      " [-0.03956848  0.09376051  0.08544897]]\n"
     ]
    }
   ],
   "source": [
    "# 最適モデルの読み込み\n",
    "model = keras.models.load_model('./model/best_model.h5')\n",
    "y_predict = model.predict(X_test_n)\n",
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自作のコールバック関数\n",
    "\n",
    "コールバック関数は、自作したものを使用することもできる。\n",
    "\n",
    "今回は、EPOCH毎に、精度を描画するコールバック関数を実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        # 各epoch毎にlossを格納していく\n",
    "        self.loss = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \"\"\"各epoch終了ごとに呼び出される\n",
    "        \"\"\"\n",
    "        # lossを格納\n",
    "        self.loss.append(logs['loss'])\n",
    "        \n",
    "        # グラフ初期化\n",
    "        clear_output(wait = True)\n",
    "        \n",
    "        # グラフ描画部\n",
    "        plt.figure(num=1, clear=True)\n",
    "        plt.title('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.plot(self.loss, label='train')\n",
    "        plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 必要ライブラリのインポート\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# モデル生成\n",
    "model = Sequential()\n",
    "\n",
    "# 層の追加\n",
    "layers=[\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.01),\n",
    "    normalization.BatchNormalization(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.01),\n",
    "    normalization.BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(3, activation='linear')\n",
    "]\n",
    "for layer in layers:\n",
    "    model.add(layer)\n",
    "\n",
    "# モデルの学習設定\n",
    "\n",
    "model.compile(\n",
    "    loss=losses.mean_squared_error,\n",
    "    optimizer=optimizers.Adam(),\n",
    "    metrics=['acc']\n",
    ")\n",
    "\n",
    "# モデルの学習\n",
    "result = model.fit(\n",
    "    X_train_n,\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    verbose=False,\n",
    "    validation_data=(X_test_n,y_test),\n",
    "    callbacks=[\n",
    "        LossHistory()\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
